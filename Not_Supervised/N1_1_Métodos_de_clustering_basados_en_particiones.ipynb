{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N1.1 - Métodos de clustering basados en particiones.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GZabalaG/AIMaster/blob/main/Not_Supervised/N1_1_M%C3%A9todos_de_clustering_basados_en_particiones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnyNgqjZ2J6z"
      },
      "source": [
        "<center><h1>N1: Métodos de clustering</h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxsktEaLU84"
      },
      "source": [
        "# N1.1: Métodos de clustering basados en particiones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wULs0b9p_Qxb"
      },
      "source": [
        "### RECUERDA RELLENAR TUS DATOS A CONTINUACIÓN ANTES DE HACER NADA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRh8xjQn_Rgo"
      },
      "source": [
        "# ===============================================================#\n",
        "# Rellena AQUÍ tu nombre y apellidos antes de hacer nada\n",
        "# ===============================================================#\n",
        "\n",
        "NOMBRE = 'TuNombre'\n",
        "APELLIDOS = 'TusApellidos'\n",
        "\n",
        "# ===============================================================#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh7PKLAdUvav"
      },
      "source": [
        "En esta práctica estudiaremos el funcionamiento y la utilización métodos de clustering basados en particiones, como el K-means y el K-medoids.\n",
        "\n",
        "Para empezar, cargamos las librerías que vamos a necesitar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woFGy9D9UuxZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as it\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyYPaVkAUyBL"
      },
      "source": [
        "Cargamos el dataset con el que vamos a trabajar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJRtQNLYU3Q9"
      },
      "source": [
        "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_cuatro_diferente_densidad.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "print('El dataset cargado tiene',Dy.size,'instancias.')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fYcOugU-P0"
      },
      "source": [
        "El algoritmo K-means tiene un único parámetro: el número de clústeres (K). Una vez fijado este valor, el primer paso consiste en elegir unos centros iniciales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oss-xE0VBVB"
      },
      "source": [
        "# Elegimos un número de clústeres a buscar\n",
        "K = 3\n",
        "\n",
        "cDx = np.zeros(K*Dx.shape[1])\n",
        "cDx.shape = (K,Dx.shape[1])\n",
        "\n",
        "def random_sample_float(n, mi, ma):\n",
        "    return (ma - mi) * np.random.random_sample(n) + mi\n",
        "\n",
        "for d in np.arange(Dx.shape[1]):\n",
        "    cDx[:,d] = random_sample_float(K, np.min(Dx[:,d]), np.max(Dx[:,d]))\n",
        "\n",
        "print('Los centros iniciales elegidos aleatoriamente son:')\n",
        "print(cDx)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZGAeMreVhKB"
      },
      "source": [
        "Vamos ahora a implementar el algoritmo de K-means. ¡No sufráis! Es un algoritmo sencillo y vais a ser capaces sin problemas.\n",
        "\n",
        "Recordad que el algoritmo K-means es un proceso iterativo en el que se van ajustando los grupos para producir el resultado final. \n",
        "\n",
        "Se parte de los datos y de una inicialización aleatoria de los centros de tantos clusters como queremos encontrar, y a partir de ahí se realizan dos operaciones iterativamente:\n",
        "\n",
        "1.   Paso de asignación de datos\n",
        "\n",
        "En este paso, cada elemento de nuestro dataset se asigna al centroide más cercano basado en la distancia escogida: en nuestro caso, la Euclidiana. Por tanto, tendremos que implementar la siguiente operación:\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argminA}{arg\\,min}\n",
        "\\argminA_{c_i \\in C} dist\\left ( c_i, x \\right )^2\n",
        "$$\n",
        "\n",
        "2.   Paso de actualización del centroide\n",
        "\n",
        "En este paso se recalcula el centroide de cada cluster, teniendo en cuenta la nueva asignación calculada en el paso 1. Para ello, se calcula la media de todos los puntos asignados en el paso anterior.\n",
        "\n",
        "$$\n",
        "c_i = \\frac{1}{\\left | S_i \\right |} \\sum_{x_i \\in S_i} x_i\n",
        "$$\n",
        "\n",
        "Y esto es todo. Este proceso se repite hasta cumplir un criterio de detención:\n",
        "\n",
        "*   No hay cambios en los puntos asignados a cada cluster\n",
        "*   La suma de las distancias alcanza un valor mínimo establecido\n",
        "*   Se alcanza un número máximo de iteraciones\n",
        "\n",
        "Vamos a implementarlo. Tenéis la estructura del bucle que realiza el proceso, y simplemente tenéis que rellenar el código que falta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_2CC1h5VDAi"
      },
      "source": [
        "# Definimos la distancia euclidiana de una instancia x \n",
        "# con respecto a un grupo de instancias C\n",
        "def distancia_euclidiana_grupo(x, C):   \n",
        "    return np.sqrt(np.sum(np.power(C-x,2),axis=1))\n",
        "\n",
        "# Preparamos el vector donde guardamos la asignación de cada elemento \n",
        "# a un clúster (1,...,K)\n",
        "Dyp = np.zeros(Dx.shape[0])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')\n",
        "\n",
        "# Flag de convergencia\n",
        "iterando = True\n",
        "\n",
        "while iterando:\n",
        "\n",
        "    # Vector auxiliar para guardar los centros de la iteración pasada\n",
        "    # necesarios para identificar la convergencia\n",
        "    cDx_ant = cDx.copy()\n",
        "\n",
        "    # Buscamos el centro más cercano a cada instancia y la asignamos a ese clúster\n",
        "    for i in np.arange(Dx.shape[0]):\n",
        "        # Tenemos que calcular la distancia de cada muestra del dataset Dx[i]\n",
        "        # a cada centro existente en cDx. Podemos utilizar para ello la función\n",
        "        # distancia_euclidiana_grupo.\n",
        "        \n",
        "        # P1\n",
        "        distancia_punto_a_centros = ## P1. Tu código aquí ##\n",
        "        \n",
        "        # Una vez calculada la distancia, necesitamos averiguar el índice de\n",
        "        # la distancia mínima. Para ello, podemos usar la función argmin de numpy.\n",
        "        pred_y = np.argmin(distancia_punto_a_centros)\n",
        "        Dyp[i] = pred_y\n",
        "\n",
        "    # Calcular los nuevos centros\n",
        "    for k in range(K):\n",
        "        # Necesitamos recalcular la media de las coordenadas de los elementos \n",
        "        # asignados a cada cluster. Recordad que cDx es la matriz donde almacenamos\n",
        "        # los centroides, así que tendrá la forma (k, 2).\n",
        "\n",
        "        # Para calcular la media de los elementos de un cluster, primero necesitamos\n",
        "        # saber qué elementos pertenecen a ese cluster, lo cual podemos obtener usando\n",
        "        # `Dyp` y `k`. Esto nos devolverá un vector de booleanos que podremos usar con\n",
        "        # `Dx` para obtener las coordenadas de los elementos del cluster `k`. Una vez\n",
        "        # disponemos de dichas coordenadas, simplemente tenemos que calcular la media.\n",
        "        # Tened en cuenta que `np.mean` reduce la media a 1 único valor, y nosotros \n",
        "        # necesitamos 2 valores, uno para la coordenada x y otro para la y.\n",
        "        # Podéis usar el argumento `axis` para indicarle a np.mean sobre qué dimensión\n",
        "        # de la matriz de puntos `Dx` queréis realizar la media.\n",
        "        \n",
        "        # P2\n",
        "        cDx[k,:] = ## P2. Tu código aquí ##\n",
        "\n",
        "    for k in np.arange(K):\n",
        "        ax.plot( [cDx_ant[k,0], cDx[k,0]],[cDx_ant[k,1], cDx[k,1]], linestyle='-', marker='*', c='y')\n",
        "    \n",
        "    iterando = (np.absolute(np.sum(cDx-cDx_ant)) > 0.00001)\n",
        "\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')\n",
        "\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp)\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZr3h_LebjOc"
      },
      "source": [
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "def medida_error(mat):\n",
        "    maxs = np.zeros(mat.shape[0])\n",
        "    for l in np.arange(mat.shape[0]):\n",
        "        maxs[l] = np.max(mat[l, :])\n",
        "    maxs = np.sum(maxs)\n",
        "    return 1 - maxs / np.sum(mat)\n",
        "\n",
        "def medida_pureza(mat):\n",
        "    totales = np.sum(mat, 0) / np.sum(mat)\n",
        "    precision_kl = np.zeros(mat.shape[1])\n",
        "    for k in np.arange(mat.shape[1]):\n",
        "        precision_kl[k] = np.max(mat[:, k] / np.sum(mat[:, k]))\n",
        "    return np.sum(totales * precision_kl)\n",
        "\n",
        "def medida_precision(mat, l, k):\n",
        "    return mat[l,k]/np.sum(mat[:,k])\n",
        "\n",
        "def medida_recall(mat, l, k):\n",
        "    return mat[l,k]/np.sum(mat[l,:])\n",
        "\n",
        "def medida_f1_especifica(mat, l, k):\n",
        "    prec = medida_precision(mat, l, k)\n",
        "    rec = medida_recall(mat, l, k)\n",
        "    if (prec+rec)==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2*prec*rec/(prec+rec)\n",
        "\n",
        "def medida_f1(mat):\n",
        "    totales = np.sum(mat, 1) / np.sum(mat)\n",
        "    f1 = np.zeros(mat.shape)\n",
        "    for l in np.arange(mat.shape[0]):\n",
        "        for k in np.arange(mat.shape[1]):\n",
        "            f1[l, k] = medida_f1_especifica(mat, l, k)\n",
        "    f1 = np.sum(totales * np.max(f1, axis=1))\n",
        "    return f1\n",
        "\n",
        "mC = contingency_matrix(Dy,Dyp)\n",
        "mE = medida_error(mC)\n",
        "mP = medida_pureza(mC)\n",
        "mF1 = medida_f1(mC)\n",
        "\n",
        "print('Matriz de confusión:\\n', mC)\n",
        "print(f'El valor del error cometido es = {mE}')\n",
        "print(f'La pureza del agrupamiento obtenido es = {mP}')\n",
        "print(f'El valor F1 es = {mF1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N-j3S5GdpGY"
      },
      "source": [
        "El algoritmo de K-means está también disponible en sklearn, así que... ¿qué os parece si comprobamos si hemos hecho bien el trabajo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8RqtTRqdoee"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Se inicializa KMeans con el número de clústeres a buscar\n",
        "modelo = KMeans(n_clusters=3, init=cDx)\n",
        "# Se aprende el \n",
        "modelo = modelo.fit(Dx)\n",
        "# Predicting the clusters\n",
        "Dyp_sk = modelo.predict(Dx)\n",
        "# Obtener los centros de los clústeres\n",
        "cDx_sk = modelo.cluster_centers_\n",
        "\n",
        "# Comparing with scikit-learn centroids\n",
        "print('Centros encontrados por...')\n",
        "print('el método programado:')\n",
        "print(cDx)\n",
        "print('el método de Sci-kit Learn:')\n",
        "print(cDx_sk)\n",
        "if np.allclose(cDx, cDx_sk):\n",
        "  print('\\n\\nBuen trabajo, tu implementación es correcta!\\n\\n')\n",
        "else:\n",
        "  print('\\n\\nHay algún tipo de error en tu implementación, ¡fíjate bien!\\n\\n')\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp_sk)\n",
        "ax.scatter(cDx_sk[:,0],cDx_sk[:,1], marker='*', s=200, c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "les9ctjUcXzi"
      },
      "source": [
        "<hr>\n",
        "<h2>K-mediods</h2>\n",
        "\n",
        "K-medoids es un método de clustering muy similar a K-means en cuanto a que ambos agrupan las observaciones en K clusters, donde K es un valor preestablecido por el analista. \n",
        "\n",
        "La diferencia es que, en K-medoids, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular.\n",
        "\n",
        "Esto pasa habitualmente cuando los datos son heterogeneos (una mezcla de variables continuas y categóricas) y los centros (medias) no se pueden calcular.\n",
        "\n",
        "Una definición más exacta del término medoid es: elemento dentro de un cluster cuya distancia (diferencia) promedio entre él y todos los demás elementos del mismo cluster es lo menor posible. Se corresponde con el elemento más central del cluster y por lo tanto puede considerarse como el más representativo. El hecho de utilizar medoids en lugar de centroides hace de K-medoids un método más robusto que K-means, viéndose menos afectado por outliers o ruido. A modo de idea intuitiva puede considerarse como la analogía entre media y mediana.\n",
        "\n",
        "Como hemos visto, el algoritmo se plantea de manera equivalente a K-means, pero la selección de centros se realiza sobre el conjunto de datos de entrenamiento. Sólo con el objetivo de poder visualizar los resultados, usaremos un conjunto de datos continuo:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLY2saTEcavS"
      },
      "source": [
        "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_cuatro_separables_peque.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "print('El dataset cargado tiene',Dy.size,'instancias.')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfqFRiWcccVb"
      },
      "source": [
        "# Elegimos un número de clústeres a buscar\n",
        "K = 3\n",
        "\n",
        "# Elegimos los centros iniciales entre el conjunto de datos\n",
        "cDx = Dx[np.random.choice(Dx.shape[0], K, replace=False),:]\n",
        "\n",
        "print('Los centros iniciales elegidos aleatoriamente son:')\n",
        "print(cDx)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')\n",
        "\n",
        "# Definimos la distancia euclidiana de una instancia x \n",
        "# con respecto a un grupo de instancias C\n",
        "def distancia_euclidiana_grupo(x, C):   \n",
        "    return np.sqrt(np.sum(np.power(C-x,2),axis=1))\n",
        "\n",
        "def distancia_euclidiana(x, y):   \n",
        "    return np.sqrt(np.sum((x-y)**2))\n",
        "\n",
        "def matriz_distancias(X, distancia):\n",
        "    mD = np.zeros((X.shape[0],X.shape[0]))\n",
        "    for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
        "        mD[pair] = distancia(X[pair[0],:],X[pair[1],:])\n",
        "    return mD\n",
        "\n",
        "# Preparamos el vector donde guardamos la asignación de cada elemento \n",
        "# a un clúster (1,...,K)\n",
        "Dyp = np.zeros(Dx.shape[0])\n",
        "\n",
        "# Flag de convergencia\n",
        "iterando = True\n",
        "\n",
        "while iterando:\n",
        "\n",
        "    # Vector auxiliar para guardar los centros de la iteración pasada\n",
        "    # necesarios para identificar la convergencia\n",
        "    cDx_ant = cDx.copy()\n",
        "\n",
        "    # Buscamos el centro más cercano a cada instancia y la asignamos a ese clúster\n",
        "    for i in np.arange(Dx.shape[0]):\n",
        "        # Tenemos que calcular la distancia de cada muestra del dataset Dx[i]\n",
        "        # a cada centro existente en cDx. Podemos utilizar para ello la función\n",
        "        # distancia_euclidiana_grupo.\n",
        "        \n",
        "        # P3\n",
        "        distancia_punto_a_centros = ## P3. Tu código aquí ##\n",
        "        \n",
        "        # Una vez calculada la distancia, necesitamos averiguar el índice de\n",
        "        # la distancia mínima. Para ello, podemos usar la función argmin de numpy.\n",
        "        pred_y = np.argmin(distancia_punto_a_centros)\n",
        "        Dyp[i] = pred_y\n",
        "\n",
        "    # Calcular los nuevos centros\n",
        "    for k in range(K):\n",
        "        # En este caso lo que buscamos es el elemento del cluster que reduce la\n",
        "        # distancia con respecto a todos los demás elementos. Para ello, calculamos\n",
        "        # las distancias entre todos los elementos del cluster `k` y las almacenamos\n",
        "        # en `mat`. \n",
        "\n",
        "        # Primero, obtenemos un vector binario indicando qué elementos de Dyp pertenecen a k\n",
        "        indices_elementos_k = Dyp==k\n",
        "\n",
        "        # Luego calculamos las distancias entre cada uno de los elementos de `Dx` \n",
        "        # que nuestra predicción dice que pertenecen al custer `k` y los demás.\n",
        "        # Para ello, podemos usar la variable `indices_elementos_k` calculada\n",
        "        # anteriormente para seleccionar los índices de `Dx` pertenecientes a `k`.\n",
        "        \n",
        "        # P4\n",
        "        mat = ## P4. Tu código aquí ##\n",
        "        \n",
        "        # Ahora buscamos el elemento con menor distancia dentro del \n",
        "        # cluster `k` y nos quedamos con su índice (usando np.argmin, que busca\n",
        "        # el mínimo de un vector y nos devuelve su índice).\n",
        "        \n",
        "        # P5\n",
        "        ic = ## P5. Tu código aquí ##\n",
        "\n",
        "        # Asignamos como centro del cluster al elemento del cluster que ofrece\n",
        "        # la menor distancia con respecto a todos los demás\n",
        "        cDx[k,:] = Dx[ic,:]\n",
        "\n",
        "    # Se muestra el desplazamiento de los centroides\n",
        "    for k in np.arange(K):\n",
        "        ax.plot( [cDx_ant[k,0], cDx[k,0]],[cDx_ant[k,1], cDx[k,1]], linestyle='-', marker='*', c='y')\n",
        "    \n",
        "    iterando = (np.absolute(np.sum(cDx-cDx_ant)) > 0.00001)\n",
        "\n",
        "# Se muestran los centroides finales\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')\n",
        "\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp)\n",
        "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htp1yQK_fdkA"
      },
      "source": [
        "<hr>\n",
        "<h2>Elegir el mejor valor de K</h2>\n",
        "\n",
        "Para elegir el mejor valor de K se suele usar la técnica del codo. Ésta consiste en probar diferentes valores de K y evaluar el agrupamientos según alguna medida de evaluación intrínseca (ya que se supone que no se conoce la verdad básica). En este ejemplo, se usan dos medidas diferentes: la medida de Silueta y la R cuadrado.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leNlTsPhffO8"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def medida_R_cuadrado(X, Xyp, cXs):\n",
        "    centros_dataset = np.mean(X,axis=0) \n",
        "    n_clusters = np.sort(np.unique(Xyp))\n",
        "    distancia_a_centros_dataset = np.sum((X-centros_dataset)**2) \n",
        "    distancias_intracluster = np.zeros(n_clusters.size)\n",
        "    for k in np.arange(n_clusters.size):\n",
        "        distancias_intracluster[k] = np.sum((X[Xyp==n_clusters[k], :] - cXs[n_clusters[k], :])**2)\n",
        "    distancia_intracluster = np.sum(distancias_intracluster)\n",
        "    # P6\n",
        "    return ## P6. Tu código aquí ##\n",
        "\n",
        "rsilueta = np.zeros(9)\n",
        "rrsquare = np.zeros(9)\n",
        "for k in np.arange(2,11):\n",
        "    modelo = KMeans(n_clusters=k, random_state=2)\n",
        "    modelo = modelo.fit(Dx)\n",
        "    Dyp_sk = modelo.predict(Dx)\n",
        "    cDx_sk = modelo.cluster_centers_\n",
        "    rsilueta[k-2] = silhouette_score(Dx, Dyp_sk)\n",
        "    rrsquare[k-2] = medida_R_cuadrado(Dx, Dyp_sk, cDx_sk)\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "ax[0].plot( np.arange(2,11),rsilueta, linestyle='-', marker='o')\n",
        "ax[0].set_xlabel(\"Número de clústeres\")\n",
        "ax[0].set_ylabel(\"Medida de ancho de silueta\")\n",
        "\n",
        "ax[1].plot( np.arange(2,11),rrsquare, linestyle='-', marker='o')\n",
        "ax[1].set_xlabel(\"Número de clústeres\")\n",
        "ax[1].set_ylabel(\"Medida de R cuadrado\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGw54GPTfhJK"
      },
      "source": [
        "Se observa que el punto con mayor puntuación es el K=4 en el caso de ancho de silueta. Con esto, podría ser suficiente. Sin embargo, si nos fijamos en la medida R cuadrado, se observa un marcado cambio de tendencia también en K=4. Este cambio es conocido como el codo. La práctica recomendada de este procedimiento indica que se debe seleccionar el punto donde se produzca el cambio de tendencia (el codo). \n",
        "\n",
        "Así, no cabe duda de que el mejor valor de K posible es 4, de acuerdo a ambas medidas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gq_h8i9JgXZ"
      },
      "source": [
        "<hr>\n",
        "<h2>K-means++</h2>\n",
        "\n",
        "Como ya sabemos, rel resultado de un clustering mediante K-means depende altamente de la inicialización de los centroides. La versión \"vainilla\" del k-means realiza esta inicialización de forma aleatoria, pero existen mejores formas de hacerlo.\n",
        "\n",
        "En este apartado vamos a ver cómo lo hace el k-means++."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19k9mWpUJ5Lp"
      },
      "source": [
        "np.random.seed(31) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "# Os recomiendo que cambiéis el seed y observéis como cambian los resultados\n",
        "# Por ejemplo, con seed = 31, encontramos una agrupación mejor que con seed = 42\n",
        "\n",
        "# Es importante que cada vez que ejecutéis el k-means++ re-ejecutéis todas las celdas\n",
        "# desde esta incluída\n",
        "\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_cuatro_diferente_densidad.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "print('El dataset cargado tiene',Dy.size,'instancias.')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwiBj1jDKIyV"
      },
      "source": [
        "El algoritmo K-means tiene un único parámetro: el número de clústeres (K). Una vez fijado este valor, el primer paso consiste en elegir unos centros iniciales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3dZpFprKKNp"
      },
      "source": [
        "# Elegimos un número de clústeres a buscar\n",
        "K = 4\n",
        "\n",
        "# Definimos la distancia euclidiana de una instancia x \n",
        "# con respecto a un grupo de instancias C\n",
        "def distancia_euclidiana_grupo(x, C):   \n",
        "    return np.sqrt(np.sum(np.power(C-x,2),axis=1))\n",
        "\n",
        "cDx = np.zeros(Dx.shape[1])\n",
        "cDx.shape = (1,Dx.shape[1])\n",
        "\n",
        "icenter = np.random.randint(Dx.shape[0])\n",
        "cDx[0,:] = Dx[icenter,:]\n",
        "auxDx = np.delete(Dx, icenter, 0)\n",
        "print(auxDx.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWfV7dJ7KQI5"
      },
      "source": [
        "Una vez elegida aleatoriamente una instancia como primer centro, procedemos a muestrear, iterativamente, un nuevo centro entre el resto del conjunto de datos de manera proporcional a la distancia a los centros ya escogidos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVvmhAWCKQqj"
      },
      "source": [
        "for k in np.arange(K-1):\n",
        "    dist_min = np.zeros(auxDx.shape[0])\n",
        "    for i in np.arange(auxDx.shape[0]):\n",
        "        # Aquí calculamos la distancia de cada punto a todos los centros\n",
        "        distancias_punto_a_centros = distancia_euclidiana_grupo(auxDx[i,:], cDx)\n",
        "        # Y necesitamos quedarnos con la mínima\n",
        "        dist_min[i] = np.min(distancias_punto_a_centros)\n",
        "    # Ahora calculamos la probabilidad de acuerdo a la fórmula vista en las transparencias\n",
        "    # P7\n",
        "    probs = ## P7. Tu código aquí ##\n",
        "    cumprobs = np.cumsum(probs)\n",
        "    icenter = np.where(cumprobs >= np.random.random_sample())[0][0]\n",
        "    cDx= np.append(cDx, np.matrix(auxDx[icenter,:]), axis=0)\n",
        "    auxDx = np.delete(auxDx, icenter, 0)\n",
        "\n",
        "print('Los centros iniciales elegidos aleatoriamente son:')\n",
        "print(cDx)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(np.array(cDx[:,0]),np.array(cDx[:,1]), marker='*', s=200, c='g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seI94FHQKZ5s"
      },
      "source": [
        "Una vez escogidos los centros, el algoritmo K-means continúa como es habitual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7b0O-PKKanh"
      },
      "source": [
        "# Preparamos el vector donde guardamos la asignación de cada elemento \n",
        "# a un clúster (1,...,K)\n",
        "Dyp = np.zeros(Dx.shape[0])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1])\n",
        "ax.scatter(np.array(cDx[:,0]),np.array(cDx[:,1]), marker='*', s=200, c='g')\n",
        "\n",
        "# Flag de convergencia\n",
        "iterando = True\n",
        "\n",
        "while iterando:\n",
        "\n",
        "    # Vector auxiliar para guardar los centros de la iteración pasada\n",
        "    # necesarios para identificar la convergencia\n",
        "    cDx_ant = cDx.copy()\n",
        "\n",
        "    # Buscamos el centro más cercano a cada instancia y la asignamos a ese clúster\n",
        "    for i in np.arange(Dx.shape[0]):\n",
        "        # Tenemos que calcular la distancia de cada muestra del dataset Dx[i]\n",
        "        # a cada centro existente en cDx. Podemos utilizar para ello la función\n",
        "        # distancia_euclidiana_grupo(punto, grupo).\n",
        "        \n",
        "        # P8\n",
        "        distancia_punto_a_centros = ## P8. Tu código aquí ##\n",
        "        \n",
        "        # Una vez calculada la distancia, necesitamos averiguar el índice de\n",
        "        # la distancia mínima. Para ello, podemos usar la función argmin de numpy.\n",
        "        pred_y = np.argmin(distancia_punto_a_centros)\n",
        "        Dyp[i] = pred_y\n",
        "\n",
        "    # Calcular los nuevos centros\n",
        "    for k in range(K):\n",
        "        # Necesitamos recalcular la media de las coordenadas de los elementos \n",
        "        # asignados a cada cluster. Recordad que cDx es la matriz donde almacenamos\n",
        "        # los centroides, así que tendrá la forma (k, 2).\n",
        "\n",
        "        # Para calcular la media de los elementos de un cluster, primero necesitamos\n",
        "        # saber qué elementos pertenecen a ese cluster, lo cual podemos obtener usando\n",
        "        # `Dyp` y `k`. Esto nos devolverá un vector de booleanos que podremos usar con\n",
        "        # `Dx` para obtener las coordenadas de los elementos del cluster `k`. Una vez\n",
        "        # disponemos de dichas coordenadas, simplemente tenemos que calcular la media.\n",
        "        # Tened en cuenta que `np.mean` reduce la media a 1 único valor, y nosotros \n",
        "        # necesitamos 2 valores, uno para la coordenada x y otro para la y.\n",
        "        # Podéis usar el argumento `axis` para indicarle a np.mean sobre qué dimensión\n",
        "        # de la matriz de puntos `Dx` queréis realizar la media.\n",
        "        \n",
        "        #P9\n",
        "        cDx[k,:] = ## P9. Tu código aquí ##\n",
        "\n",
        "    for k in np.arange(K):\n",
        "        ax.plot( [cDx_ant[k,0], cDx[k,0]],[cDx_ant[k,1], cDx[k,1]], linestyle='-', marker='*', c='y')\n",
        "    \n",
        "    iterando = (np.absolute(np.sum(cDx-cDx_ant)) > 0.00001)\n",
        "\n",
        "ax.scatter(np.array(cDx[:,0]),np.array(cDx[:,1]), marker='*', s=200, c='r')\n",
        "\n",
        "# Ver asignaciones finales\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp)\n",
        "ax.scatter(np.array(cDx[:,0]),np.array(cDx[:,1]), marker='*', s=200, c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6MRaFYZfj8M"
      },
      "source": [
        "<hr>\n",
        "<h2>Crea tus propios datasets de ejemplo</h2>\n",
        "\n",
        "Python implementa la función make_blobs, que permite generar conjuntos de datos de manera sencilla especificando simplemente el número de ejemplos (n_samples), el número de dimensiones (n_features) y el número de clústeres (centers):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXnlYiU8fgtN"
      },
      "source": [
        "# Creamos un dataset con 3 dimensiones y 4 clústeres\n",
        "Dx, Dy = make_blobs(n_samples=800, n_features=3, centers=4)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = Axes3D(fig)\n",
        "ax.scatter(Dx[:, 0], Dx[:, 1], Dx[:, 2], c=Dy)\n",
        "ax.set_title(\"Clústeres y asignaciones reales\")\n",
        "\n",
        "modelo = KMeans(n_clusters=3)\n",
        "modelo = modelo.fit(Dx)\n",
        "Dyp_sk = modelo.predict(Dx)\n",
        "cDx_sk = modelo.cluster_centers_\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = Axes3D(fig)\n",
        "ax.scatter(Dx[:, 0], Dx[:, 1], Dx[:, 2], c=Dyp_sk)\n",
        "ax.scatter(cDx_sk[:, 0], cDx_sk[:, 1], cDx_sk[:, 2], marker='*', c='r', s=1000)\n",
        "ax.set_title(\"Clústeres y asignaciones predichos\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}