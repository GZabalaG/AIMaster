{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N1.2 - Métodos de clustering jerárquico.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GZabalaG/AIMaster/blob/main/Not_Supervised/N1_2_M%C3%A9todos_de_clustering_jer%C3%A1rquico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnyNgqjZ2J6z"
      },
      "source": [
        "<center><h1>N1: Métodos de clustering</h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iE9XTGLU2V"
      },
      "source": [
        "# N1.2: Métodos de clustering jerárquico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTC8g4yMCFhh"
      },
      "source": [
        "# RECUERDA RELLENAR TUS DATOS A CONTINUACIÓN ANTES DE HACER NADA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75gqUvMQCGev"
      },
      "source": [
        "# ===============================================================#\n",
        "# Rellena AQUÍ tu nombre y apellidos antes de hacer nada\n",
        "# ===============================================================#\n",
        "\n",
        "NOMBRE = 'Gonzalo'\n",
        "APELLIDOS = 'Zabala García'\n",
        "\n",
        "# ===============================================================#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpRUPzfm-ZM8"
      },
      "source": [
        "En esta práctica vamos a ver cómo funcionan los algoritmos jerárquicos vistos en clase: el aglomerativo y el divisivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-2-Apoz-4s_"
      },
      "source": [
        "### Clustering jerárquico aglomerativo\n",
        "\n",
        "En esta práctica estudiaremos el funcionamiento y la utilización del clústering jerárquico aglomerativo.\n",
        "\n",
        "Para empezar, cargamos las librerías que vamos a necesitar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyER1UIejx76"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as it\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "plt.rcParams['figure.figsize'] = [8, 8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9bx8Km_jx79"
      },
      "source": [
        "\n",
        "Para comenzar, cargamos el conjunto de datos con el que trabajaremos:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmc-Ki7ijx7-"
      },
      "source": [
        "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_reducido.csv'\n",
        "Dx = np.array(pd.read_csv(data_file_url,header=0))\n",
        "Dx = Dx[ np.random.choice(np.arange(Dx.shape[0]), Dx.shape[0], replace=False) ,:]\n",
        "print('Tamaño de los datos: ', Dx.shape)\n",
        "\n",
        "plt.scatter(Dx[:,0], Dx[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvpfONxmjx8A"
      },
      "source": [
        "\n",
        "Con este dataset trabajaremos y estudiaremos en esta práctica las diferentes variantes del clústering aglomerativo. Para empezar, será necesario calcular la matriz de distancias, por lo que recuperamos de prácticas anteriores la función matriz_distancias.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URnHyF1jx8B"
      },
      "source": [
        "def distancia_euclidiana(x, y):   \n",
        "    return np.sqrt(np.sum((x-y)**2))\n",
        "\n",
        "def matriz_distancias(X, distancia):\n",
        "    mD = np.zeros((X.shape[0],X.shape[0]))\n",
        "    for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
        "        mD[pair] = distancia(X[pair[0],:],X[pair[1],:])\n",
        "    return mD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opg0W6mojx8D"
      },
      "source": [
        "\n",
        "Necesitamos una función que, dada una matriz de distancias, construya un clustering aglomerativo. Es decir, partiendo de n clústeres (uno por ejemplo de entrenamiento), se van uniendo iterativamente dos clústeres (escogidos según minimizan la distancia interclúster definida de acuerdo a cierto criterio) hasta que todos los elementos se agrupan en un único clúster final.\n",
        "\n",
        "Sabemos que el número de uniones es igual a n-1 (el número de ejemplos menos uno). En la siguiente función, vamos guardando en cada columna de la matriz mParticiones la partición en clústeres de los ejemplos: cada partición mParticiones[:,p], de longitud n, guarda el clúster al que pertenece el i-ésimo ejemplo en la posición mParticiones[i,p]. Para rellenar esa matriz, en cada paso (unión), se calcula la matriz de distancia entre todos los pares de clústeres usando el criterio elegido. Después, simplemente se trata de asignar todos los elementos de los clústeres elegidos a un mismo grupo. Este procedimiento iterativo se repite hasta que sólo queda un clúster en el paso final:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TPuNbjjx8D"
      },
      "source": [
        "def clustering_aglomerativo(mD, criterio):\n",
        "    # En esta matriz almacenaremos las sucesivas particiones (clusterings) de los\n",
        "    # datos. Sucesivas, porque cada paso del bucle de abajo realizará una unión\n",
        "    # entre los dos clusters más similares, hasta que no queden elementos sueltos.\n",
        "    mParticiones = np.zeros((mD.shape[0], mD.shape[0]), dtype = np.int8)\n",
        "    # Rellenamos la última columna con valores del 0 al 21, ya que en este paso \n",
        "    # cada elemento pertenecerá a un cluster distinto\n",
        "    mParticiones[:,21] = np.arange(22)\n",
        "\n",
        "    # Recorremos `mParticiones` de manera decreciente guardando los agrupamientos,\n",
        "    # ya que sabemos que el número de uniones es exactamente de n-1\n",
        "    for n in range(mParticiones.shape[1]-2, -1, -1):\n",
        "        print(f'Paso número {n}')\n",
        "    \n",
        "        # Al empezar, asignamos el agrupamiento anterior al actual, ya que el\n",
        "        # agrupamiento en el momento anterior (`n+1`) únicamente sufrirá alguna \n",
        "        # modificación para juntar los dos clusters que mejor cumplan nuestro\n",
        "        # criterio (min, max, mean).\n",
        "        mParticiones[:,n] = mParticiones[:,n+1]\n",
        "\n",
        "        # Obtenemos las \"etiquetas\" del agrupamiento actual (desde 0 hasta K, \n",
        "        # siendo K el número de clusters)\n",
        "        clust_actuales = np.unique(mParticiones[:,n])\n",
        "\n",
        "        # Y calculamos la matriz distancia\n",
        "        mDC = criterio(mD, clust_actuales, mParticiones[:,n])\n",
        "\n",
        "        # Con la información de la matriz distancias somos capaces de ver qué\n",
        "        # dos clusters son más parecidos. Necesitamos quedarnos no con el valor,\n",
        "        # si no con el índice (el cual nos va a indicar qué dos clusters `i` y `j`\n",
        "        # son los más parecidos). Para ello, usaremos la función `np.argmin`.\n",
        "        \n",
        "        # P1\n",
        "        ind_mDC_mejores_clusters = ## P1. Tu código aquí ##\n",
        "        \n",
        "        print(f'Indice de los clusters que coinciden mejor: {ind_mDC_mejores_clusters}')\n",
        "\n",
        "        # Fijaos que esta función devuelve un único índice, que es el índice \n",
        "        # considerando que la matriz es un vector de 1D. Así que nosotros tenemos\n",
        "        # que averiguar con qué `i` y `j` se corresponde dicho índice. Para ello,\n",
        "        # necesitamos la función `np.unravel_index`.\n",
        "        (cluster_i, cluster_j) = np.unravel_index(ind_mDC_mejores_clusters, mDC.shape)\n",
        "        print(f'Índices de los clusters a unir (i, j): {(cluster_i, cluster_j)}')\n",
        "\n",
        "        # Vamos a asignar a uno de los clusters (`j`) la \"etiqueta\" del otro (`i`)\n",
        "        etiqueta_cluster_i = clust_actuales[cluster_i]  # \"etiqueta\" del cluster `i`\n",
        "        # Encontramos qué elementos pertenecen al cluster `j`\n",
        "        elementos_pertenecientes_a_cluster_j = mParticiones[:, n] == clust_actuales[cluster_j]\n",
        "        # Y les asignamos la etiqueta del cluster `i`\n",
        "        \n",
        "        # P2\n",
        "        mParticiones[elementos_pertenecientes_a_cluster_j, n] = ## P2. Tu código aquí ##\n",
        "        \n",
        "        print(f'Cluster {cluster_i} y {cluster_j} asignados a etiqueta {etiqueta_cluster_i}')\n",
        "\n",
        "    return mParticiones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd1JQGoHjx8F"
      },
      "source": [
        "\n",
        "En la parte teórica estudiamos tres criterios diferentes (medidas de disimilitud interclúster). El primero que vamos a ver es el de la disimilitud media: dado cierto par de clústeres, se calcula la media de la disimilitud-distancia entre todos los pares de ejemplos (uno ejemplo de cada clúster).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-lmuJ68jx8G"
      },
      "source": [
        "def disimilitud_media(mD, clust_actuales, particion):\n",
        "\n",
        "    # Creamos una matriz para alojar las disimilitudes entre cada uno de\n",
        "    # los clusters existentes (clust_actuales.size)\n",
        "    mDC = np.zeros((clust_actuales.size, clust_actuales.size))\n",
        "\n",
        "    # Calculamoos la distancia MEDIA entre todos los elementos del cluster `n1`\n",
        "    # y todos los del cluster `n2`\n",
        "    for n1 in np.arange(clust_actuales.size):\n",
        "        # Encontramos los índices de los elementos que pertenecen al cluster `n1`\n",
        "        exC1 = np.where(particion==clust_actuales[n1])[0]\n",
        "        # Empezamos el bucle para recorrer todos los clusters existentes para\n",
        "        # poder compararlos con el cluster `n1`\n",
        "        for n2 in np.arange(clust_actuales.size):\n",
        "            # Encontramos los índices de los elementos del cluster `n2`\n",
        "            exC2 = np.where(particion==clust_actuales[n2])[0]\n",
        "            # Usando la matriz distancias `mD`, calculamos la distancia MEDIA \n",
        "            # entre un cluster y el otro. Aquí, `np._ix` simplemente permite\n",
        "            # que podamos usar `exC1` y `exC2` como índices para coger todos\n",
        "            # los elementos de C1 y C2\n",
        "            indices_C1_C2 = np.ix_(exC1, exC2)\n",
        "            # Aplicamos el criterio usando para ello los índices_C1_C2, que \n",
        "            # consisten en tuplas (i, j), y seleccionaran las distancias\n",
        "            # pertinentes de la matriz distancias `mD`. Luego aplicamos la \n",
        "            # función de numpy que calcula la media.\n",
        "            \n",
        "            # P3\n",
        "            mDC[n1,n2] = ## P3. Tu código aquí ##\n",
        "            \n",
        "    # Rellenamos la matriz diagonal con valores elevados (el máximo de la matriz \n",
        "    # elevado al cuadrado) para evitar que se proponga la unión de un clúster \n",
        "    # consigo mismo.\n",
        "    np.fill_diagonal(mDC, np.max(mDC)*2)\n",
        "\n",
        "    return mDC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFBl5uLnjx8I"
      },
      "source": [
        "\n",
        "Con pocos cambios, se puede definir la disimilitud interclúster mínima: dado cierto par de clústeres, la distancia entre ambos es igual a la distancia mínima de cualquier par de ejemplos (uno ejemplo de cada clúster).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOGKXrk8jx8I"
      },
      "source": [
        "def disimilitud_minima(mD, clust_actuales, particion):\n",
        "\n",
        "    # Creamos una matriz para alojar las disimilitudes entre cada uno de\n",
        "    # los clusters existentes (clust_actuales.size)\n",
        "    mDC = np.zeros((clust_actuales.size, clust_actuales.size))\n",
        "\n",
        "    # Calculamoos la distancia MEDIA entre todos los elementos del cluster `n1`\n",
        "    # y todos los del cluster `n2`\n",
        "    for n1 in np.arange(clust_actuales.size):\n",
        "        # Encontramos los índices de los elementos que pertenecen al cluster `n1`\n",
        "        exC1 = np.where(particion==clust_actuales[n1])[0]\n",
        "        # Empezamos el bucle para recorrer todos los clusters existentes para\n",
        "        # poder compararlos con el cluster `n1`\n",
        "        for n2 in np.arange(clust_actuales.size):\n",
        "            # Encontramos los índices de los elementos del cluster `n2`\n",
        "            exC2 = np.where(particion==clust_actuales[n2])[0]\n",
        "            # Usando la matriz distancias `mD`, calculamos la distancia MÍNIMA \n",
        "            # entre un cluster y el otro. Aquí, `np._ix` simplemente permite\n",
        "            # que podamos usar `exC1` y `exC2` como índices para coger todos\n",
        "            # los elementos de C1 y C2\n",
        "            indices_C1_C2 = np.ix_(exC1, exC2)\n",
        "            # Aplicamos el criterio usando para ello los índices_C1_C2, que \n",
        "            # consisten en tuplas (i, j), y seleccionaran las distancias\n",
        "            # pertinentes de la matriz distancias `mD`. Luego aplicamos la \n",
        "            # función de numpy que calcula el mínimo.\n",
        "            \n",
        "            # P4\n",
        "            mDC[n1,n2] = ## P4. Tu código aquí ##\n",
        "            \n",
        "    # Rellenamos la matriz diagonal con valores elevados (el máximo de la matriz \n",
        "    # elevado al cuadrado) para evitar que se proponga la unión de un clúster \n",
        "    # consigo mismo.\n",
        "    np.fill_diagonal(mDC, np.max(mDC)*2)\n",
        "    return mDC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMBulq_9jx8K"
      },
      "source": [
        "\n",
        "De manera equivalente, se define la disimilitud interclúster máxima: dado cierto par de clústeres, la distancia entre ambos es igual a la distancia máxima de cualquier par de ejemplos (uno ejemplo de cada clúster).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yAwDTCWjx8L",
        "scrolled": false
      },
      "source": [
        "def disimilitud_maxima(mD, clust_actuales, particion):\n",
        "\n",
        "    # Creamos una matriz para alojar las disimilitudes entre cada uno de\n",
        "    # los clusters existentes (clust_actuales.size)\n",
        "    mDC = np.zeros((clust_actuales.size, clust_actuales.size))\n",
        "\n",
        "    # Calculamoos la distancia MEDIA entre todos los elementos del cluster `n1`\n",
        "    # y todos los del cluster `n2`\n",
        "    for n1 in np.arange(clust_actuales.size):\n",
        "        # Encontramos los índices de los elementos que pertenecen al cluster `n1`\n",
        "        exC1 = np.where(particion==clust_actuales[n1])[0]\n",
        "        # Empezamos el bucle para recorrer todos los clusters existentes para\n",
        "        # poder compararlos con el cluster `n1`\n",
        "        for n2 in np.arange(clust_actuales.size):\n",
        "            # Encontramos los índices de los elementos del cluster `n2`\n",
        "            exC2 = np.where(particion==clust_actuales[n2])[0]\n",
        "            # Usando la matriz distancias `mD`, calculamos la distancia MÁXIMA \n",
        "            # entre un cluster y el otro. Aquí, `np._ix` simplemente permite\n",
        "            # que podamos usar `exC1` y `exC2` como índices para coger todos\n",
        "            # los elementos de C1 y C2\n",
        "            indices_C1_C2 = np.ix_(exC1, exC2)\n",
        "            # Aplicamos el criterio usando para ello los índices_C1_C2, que \n",
        "            # consisten en tuplas (i, j), y seleccionaran las distancias\n",
        "            # pertinentes de la matriz distancias `mD`. Luego aplicamos la \n",
        "            # función de numpy que calcula el máximo.\n",
        "            \n",
        "            # P5\n",
        "            mDC[n1,n2] = ## P5. Tu código aquí ##\n",
        "            \n",
        "    # Rellenamos la matriz diagonal con valores elevados (el máximo de la matriz \n",
        "    # elevado al cuadrado) para evitar que se proponga la unión de un clúster \n",
        "    # consigo mismo.\n",
        "    np.fill_diagonal(mDC, np.max(mDC)*2)\n",
        "    return mDC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjpQcGWLjx8N"
      },
      "source": [
        "\n",
        "Finalmente, incluimos una serie de funciones que nos van a permitir visualizar los resultados:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO-ATm7Kjx8N"
      },
      "source": [
        "# Muestra los ejemplos coloreados según su pertenencia a los clústeres\n",
        "# K indica el número de clústeres a mostrar a partir de la jerarquía\n",
        "def plot_particion_K(Dx, mParticiones,K):\n",
        "    cmap = get_cmap('tab20')\n",
        "    vals = np.arange(Dx.shape[0]+2)/(Dx.shape[0]+2)\n",
        "    rgba = cmap(vals[np.arange(Dx.shape[0])+1])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,5))\n",
        "    ax.scatter(Dx[:,0],Dx[:,1], c=rgba[mParticiones[:,K-1],:])\n",
        "\n",
        "# Muestra el plot de todas las particiones posibles de la jerarquía\n",
        "def plot_particiones(Dx, mParticiones):\n",
        "    for k in np.arange(mParticiones.shape[1]):\n",
        "        plot_particion_K(Dx, mParticiones,k+1)\n",
        "\n",
        "# Muestra el dendrograma de la jerarquía\n",
        "def plot_dendrograma_de_mparticiones(mD, mParticiones):\n",
        "    nPasos = mD.shape[0]-1\n",
        "    distancias = np.zeros(nPasos)\n",
        "    tamanyos = np.zeros(nPasos)\n",
        "    uniones = np.zeros(2*nPasos,dtype=np.int8)\n",
        "    uniones.shape=(nPasos,2)\n",
        "\n",
        "    mNParticiones = mParticiones.copy()\n",
        "    for n in np.arange(mNParticiones.shape[0]):\n",
        "        valor = mParticiones[n,mNParticiones.shape[1]-1]\n",
        "        mNParticiones[mParticiones==valor] = n\n",
        "\n",
        "    k = 0\n",
        "    aux = np.array(range(mNParticiones.shape[1]-1))\n",
        "    for n in aux[::-1]:\n",
        "        # cual es el diferente?\n",
        "        prim_diferencia = np.where(mParticiones[:,n]!=mParticiones[:,n+1])[0][0]\n",
        "        submatriz = mNParticiones[:,:n+1]\n",
        "        submatriz[submatriz==mNParticiones[prim_diferencia,n]] = mNParticiones.shape[1]+k\n",
        "        mNParticiones[:,:n+1] = submatriz\n",
        "        uniones[k,:] = np.unique(mNParticiones[mNParticiones[:,n]==mNParticiones.shape[1]+k,n+1])\n",
        "        tamanyos[k] = np.sum(mNParticiones[:,n]==mNParticiones.shape[1]+k)\n",
        "\n",
        "        vs = mNParticiones[:,n+1]\n",
        "        distancias[k] = 1 / (2 * tamanyos[k]) * (    \n",
        "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,0])[0],\n",
        "                              np.where(vs == uniones[k,1])[0])])+\n",
        "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,1])[0],\n",
        "                              np.where(vs == uniones[k,0])[0])]))\n",
        "        k += 1\n",
        "\n",
        "    distancias = np.arange(uniones.shape[0])\n",
        "    # Creamos la matriz de enlaces que necesita el método dendrogram de scipy\n",
        "    mEnlaces = np.column_stack([uniones, distancias, tamanyos]).astype(float)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title('Dendrograma de Clustering Jerárquico')\n",
        "    plt.xlabel('Índice del caso')\n",
        "    plt.ylabel('Distancia')\n",
        "    dendrogram(mEnlaces)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic5n6c6Ejx8P"
      },
      "source": [
        "\n",
        "Ahora ya estamos en disposición de realizar el clustering jerárquico aglomerativo. Empezaremos por calcular la matriz de distancias y, por este orden, calcularemos y mostraremos el dendrograma de los diferentes agrupamientos obtenidos de usar el criterio de disimilitud mínima, máxima y media. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVTcXHA5jx8Q",
        "scrolled": false
      },
      "source": [
        "mD = matriz_distancias(Dx, distancia_euclidiana)\n",
        "print('Tamaño de la matriz distancias (para cada elemento del dataset): ', mD.shape)\n",
        "plt.imshow(mD, vmin=0, vmax=mD.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHfR8lEMZOCe"
      },
      "source": [
        "# usando el criterio de disimilitud mínima\n",
        "mParticiones_min = clustering_aglomerativo(mD, disimilitud_minima)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2y1H9BqZTrA"
      },
      "source": [
        "# usando el criterio de disimilitud máxima\n",
        "mParticiones_max = clustering_aglomerativo(mD, disimilitud_maxima)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trxyrrHKZYMT"
      },
      "source": [
        "# usando el criterio de disimilitud media\n",
        "mParticiones_mean = clustering_aglomerativo(mD, disimilitud_media)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r68rOZCjx8R"
      },
      "source": [
        "\n",
        "La función plot_particiones permite observar cómo son las diferentes particiones (niveles del clustering jerárquico) mostrando el dataset original con la asignación a clústeres descrita mediante colores: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHVDJp2Bjx8S",
        "scrolled": false
      },
      "source": [
        "plot_particiones(Dx, mParticiones_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjf3ETfNjx8U"
      },
      "source": [
        "\n",
        "El clustering jerárquico devuelve un espectro de agrupamientos. Muchas veces, es necesario seleccionar una única partición. Dada la matriz mParticiones, esto equivalente a quedarse con una de las columnas. Si queremos seleccionar una partición donde hayan K clústeres, debemos seleccionar la columna K-1. Así, podemos calcular el valor de una métrica de evaluación cualquiera. En este caso, usamos las métricas de ancho de silueta y de Calinski Harabaz:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smKU9FR_jx8V"
      },
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Queremos visualizar el clustering conseguido con K = 4. Como hemos visto\n",
        "# anteriormente, los clusterings con diferentes K se encuentran en las columnas\n",
        "# de mParticiones. Como queremos el clustering con K = 4, necesitamos la columna\n",
        "# K-1. \n",
        "# Podemos elegir mParticiones_{min/max/mean}\n",
        "mParticiones = mParticiones_mean\n",
        "K = 4\n",
        "\n",
        "# P6\n",
        "partToEval = ## P6. Tu código aquí ##\n",
        "\n",
        "print('La medida de Silueta con K =',K,'es',silhouette_score(Dx, partToEval))\n",
        "print('La medida de Calinski Harabaz con K =',K,'es',calinski_harabasz_score(Dx, partToEval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44_UrOWEjx8X"
      },
      "source": [
        "\n",
        "Podríamos incluso dibujar de manera sencilla la figura del codo recorriendo todas las columnas:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBFNcPm0jx8X"
      },
      "source": [
        "rsilueta = np.zeros(mParticiones.shape[1])\n",
        "rch = np.zeros(mParticiones.shape[1])\n",
        "for K in np.arange(1,mParticiones.shape[1]-1):\n",
        "    rsilueta[K] = silhouette_score(Dx, mParticiones[:,K])\n",
        "    rch[K] = calinski_harabasz_score(Dx, mParticiones[:,K])\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "ax[0].plot(np.arange(1,mParticiones.shape[1]-1), rsilueta[np.arange(1,mParticiones.shape[1]-1)],\n",
        "           linestyle='-', marker='o')\n",
        "ax[0].set_xlabel(\"Número de clústeres\")\n",
        "ax[0].set_ylabel(\"Medida de ancho de silueta\")\n",
        "\n",
        "ax[1].plot(np.arange(1,mParticiones.shape[1]-1), rch[np.arange(1,mParticiones.shape[1]-1)],\n",
        "           linestyle='-', marker='o')\n",
        "ax[1].set_xlabel(\"Número de clústeres\")\n",
        "ax[1].set_ylabel(\"Medida de Calinski Harabaz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hio2fynjx8Z"
      },
      "source": [
        "<hr>\n",
        "<h2>Implementaciones en librerías de Python</h2>\n",
        "\n",
        "La librería ScikitLearn ya implementa el algoritmo de clustering jerárquico aglomerativo. \n",
        "\n",
        "Están implementados (parámetro linkage) los criterios estudiados en teoría (disimilitud mínima : 'single'; máxima: 'complete'; y media: 'average') y muchos más.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QupQznMjx8a"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "modelo = AgglomerativeClustering(linkage=\"single\")\n",
        "modelo = modelo.fit(Dx)\n",
        "\n",
        "modelo = AgglomerativeClustering(linkage=\"complete\")\n",
        "modelo = modelo.fit(Dx)\n",
        "\n",
        "modelo = AgglomerativeClustering(linkage=\"average\")\n",
        "modelo = modelo.fit(Dx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMd3qtdtjx8b"
      },
      "source": [
        "\n",
        " En cambio, para mostrar el dendrograma, tenemos que usar la library scipy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Lt7UKYjx8c",
        "scrolled": false
      },
      "source": [
        "# Creamos la matriz de enlaces que necesita el método dendrogram de scipy\n",
        "uniones = modelo.children_\n",
        "# Las distancias y los tamaños, en esta ocasión, los asignamos de manera \n",
        "# inocua para no alterar el resultado (no disponemos de la información completa)\n",
        "distancias = np.arange(uniones.shape[0])\n",
        "tamanyos = np.arange(2, uniones.shape[0]+2)\n",
        "mEnlaces = np.column_stack([uniones, distancias, tamanyos]).astype(float)\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Dendrograma de Clustering Jerárquico')\n",
        "plt.xlabel('Índice del caso')\n",
        "plt.ylabel('Distancia')\n",
        "dendrogram(mEnlaces)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzDeOHWYjx8e"
      },
      "source": [
        "\n",
        "Probablemente, la librería más completa para hacer clustering jerárquico aglomerativo es scipy. La función linkage es la encargada de hacer el agrupamiento jerárquico. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ZDd0sMjx8e"
      },
      "source": [
        "from scipy.cluster.hierarchy import linkage, fcluster, cut_tree\n",
        "\n",
        "# Podéis hacer pruebas comentando y descomentando\n",
        "#modelo = linkage(Dx, 'single')   # disimilitud mínima\n",
        "#modelo = linkage(Dx, 'complete') # disimilitud máxima\n",
        "modelo = linkage(Dx, 'average')  # disimilitud media\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Dendrograma de Clustering Jerárquico')\n",
        "plt.xlabel('Índice del caso')\n",
        "plt.ylabel('Distancia')\n",
        "dendrogram(modelo)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x45eb8tmjx8g"
      },
      "source": [
        "\n",
        "La función `cut_tree` permite obtener una partición concreta dado un número de clústeres `K`. Es lo equivalente a quedarnos con la columna K - 1 de nuestra `mParticiones`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0IP6L_Djx8h",
        "scrolled": true
      },
      "source": [
        "plt.scatter(Dx[:,0], Dx[:,1], c=cut_tree(modelo, n_clusters=4).flatten())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqatgfkjx8k"
      },
      "source": [
        "\n",
        "Con la siguiente función dibujaremos los resultados del clustering (fijado un K) usando diferentes criterios de unión. Podemos usar diferentes datasets de ejemplo (ver aquellos que tiene 2 dimensiones en:\n",
        "https://github.com/flifuehu/viu-unsupervised-learning/tree/master/datasets)\n",
        "\n",
        "Podemos hacer unas pruebas para ganar algunas intuiciones sobre cuál es la mejor estrategia según el tipo de datos...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCe7i-Axjx8m",
        "scrolled": false
      },
      "source": [
        "def plot_varios(Dx,Dy,K):\n",
        "    fig, ax = plt.subplots(1,4, figsize=(20,5))\n",
        "    ax[0].scatter(Dx[:,0], Dx[:,1], c=Dy)\n",
        "    ax[0].set_title('Datos originales')\n",
        "\n",
        "    modelo = linkage(Dx, 'single')\n",
        "    ax[1].scatter(Dx[:,0], Dx[:,1], c=cut_tree(modelo, n_clusters = K).flatten())\n",
        "    ax[1].set_title('Disimilitud mínima')\n",
        "    \n",
        "    modelo = linkage(Dx, 'complete')\n",
        "    ax[2].scatter(Dx[:,0], Dx[:,1], c=cut_tree(modelo, n_clusters = K).flatten())\n",
        "    ax[2].set_title('Disimilitud máxima')\n",
        "    \n",
        "    modelo = linkage(Dx, 'average')\n",
        "    ax[3].scatter(Dx[:,0], Dx[:,1], c=cut_tree(modelo, n_clusters = K).flatten())\n",
        "    ax[3].set_title('Disimilitud media')\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_circulos_concentricos.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "\n",
        "plot_varios(Dx,Dy,2)\n",
        "\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_dos_remolinos.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "\n",
        "plot_varios(Dx,Dy,2)\n",
        "\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_cuatro_diferente_medida.csv'\n",
        "D = np.array(pd.read_csv(data_file_url,header=0))\n",
        "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
        "Dx = D[:,0:2]\n",
        "Dy = D[:,2]\n",
        "\n",
        "plot_varios(Dx,Dy,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riebUrR2jx8t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSotLvgBdZdR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVWxUmsndZzE"
      },
      "source": [
        "# Clustering jerárquico divisivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RKlT-xIjyp6"
      },
      "source": [
        "Vamos a ver ahora el funcionamiento y la utilización del clústering jerárquico divisivo.\n",
        "\n",
        "Para empezar, cargamos las librerías que vamos a necesitar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLKE9pGSjyp7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as it\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cut_tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWDu6Injyp-"
      },
      "source": [
        "\n",
        "Para comenzar, cargamos el conjunto de datos con el que trabajaremos:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgaHpPybjyp_"
      },
      "source": [
        "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
        "\n",
        "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_reducido.csv'\n",
        "Dx = np.array(pd.read_csv(data_file_url,header=0))\n",
        "Dx = Dx[ np.random.choice(np.arange(Dx.shape[0]), Dx.shape[0], replace=False) ,:]\n",
        "\n",
        "plt.scatter(Dx[:,0], Dx[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvuSPaAVjyqB"
      },
      "source": [
        "Es el mismo dataset que usamos en el apartado anterior. Lo volveremos a usar ahora para estudiar las diferentes variantes del clústering divisivo. Empezamos nuevamente calculando la matriz de distancias, para lo que necesitaremos otra vez la función `matriz_distancias`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq_TVdnFjyqC"
      },
      "source": [
        "def distancia_euclidiana(x, y):   \n",
        "    return np.sqrt(np.sum((x-y)**2))\n",
        "\n",
        "def matriz_distancias(X, distancia):\n",
        "    mD = np.zeros((X.shape[0],X.shape[0]))\n",
        "    for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
        "        mD[pair] = distancia(X[pair[0],:],X[pair[1],:])\n",
        "    return mD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7GpXz1SjyqE"
      },
      "source": [
        "\n",
        "Necesitamos una función que, dada una matriz de distancias, construya un clustering divisivo. Es decir, partiendo de un único clúster (con todos los ejemplos de entrenamiento), se van dividiendo en dos iterativamente hasta que finalmente todos los elementos tienen su propio clúster unitario. En cada paso, el clúster a dividir se escoge como aquél que maximiza la disimilitud intraclúster (que puede ser definida de acuerdo a diferentes criterios). El clúster escogido se divide de acuerdo a cierto procedimiento de separación.\n",
        "\n",
        "Sabemos que el número de divisiones es igual a `n-1` (el número de ejemplos menos uno). En la siguiente función, vamos guardando en cada columna de la matriz `mParticiones` la partición en clústeres de los ejemplos: cada partición `mParticiones[:, p]`, de longitud `n`, guarda el clúster al que pertenece el i-*ésimo* ejemplo en la posición `mParticiones[i, p]`. \n",
        "\n",
        "Para rellenar esa matriz, en cada paso (división), se calcula la disimilitud intraclúster de todos los clústeres usando el criterio elegido y se elige un clúster a dividir (`iClusterADiv`). Después, se aplica el procedimiento de separación elegido para obtener un subconjunto de elementos de ese clúster que pasan a formar parte de un nuevo clúster (se les asigna al clúster con índice `n`). \n",
        "\n",
        "Este procedimiento iterativo se repite hasta que sólo quedan clústeres unitarios (cada clúster tiene un único elemento) en el paso final:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVSjqnBZjyqF",
        "scrolled": false
      },
      "source": [
        "def clustering_divisivo(mD, disimilitud_intracluster, separador, **kwargs):\n",
        "    # En esta matriz almacenaremos las sucesivas particiones (clusterings) de los\n",
        "    # datos. Sucesivas, porque cada paso del bucle de abajo realizará una separación\n",
        "    # entre los dos clusters más diferentes, hasta que haya un cluster por elemento\n",
        "    # del dataset.\n",
        "    mParticiones = np.zeros((mD.shape[0], mD.shape[0]), dtype = np.int8)\n",
        "\n",
        "    # Recorremos `mParticiones` (en esta ocasión, de manera creciente) guardando\n",
        "    # los agrupamientos\n",
        "    for n in range(1, mParticiones.shape[1]):\n",
        "        print(f'Paso número {n}')\n",
        "        \n",
        "        # Al empezar, asignamos el agrupamiento anterior al actual, ya que el\n",
        "        # agrupamiento en el momento anterior (`n-1`) únicamente sufrirá alguna \n",
        "        # modificación para dividir los dos clusters más separados.\n",
        "        mParticiones[:,n] = mParticiones[:,n-1]\n",
        "\n",
        "        # Calculamos la disimilitud intracluster\n",
        "        rDisIntraCluster = disimilitud_intracluster(mD, mParticiones[:, n], n)\n",
        "\n",
        "        # Y escogemos el cluster en el que mayor sea\n",
        "        \n",
        "        # P7\n",
        "        iClusterADiv = ## P7. Tu código aquí ##\n",
        "        \n",
        "        print(f'Cluster a dividir: {iClusterADiv}')\n",
        "\n",
        "        # Separamos dicho cluster en 2 sub-clusters siguiendo el criterio escogido\n",
        "        iAClusterNuevo = separador(mD, mParticiones[:, n], iClusterADiv, **kwargs)\n",
        "        print(f'Número de elementos a asignar al nuevo cluster {n}: {len(iAClusterNuevo)}')\n",
        "\n",
        "        mParticiones[iAClusterNuevo, n] = n\n",
        "\n",
        "    return mParticiones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_leO_E3jyqH"
      },
      "source": [
        "\n",
        "Como se comentaba anteriormente, la primera decisión a tomar es qué clúster se va a dividir. Para ello se evalúa la disimilitud intraclúster. Hay diferentes maneras de medir dicha disimilitud. Una de ellas es mediante el <b>diámetro</b> del clúster, que se define como la máxima distancia entre dos elementos cualquiera del clúster. La siguiente función calcula dicho valor (diámetro) para todos los clústeres y los devuelve en un array:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LeLD2FjyqI"
      },
      "source": [
        "def disimilitud_diametro(mD, particion, n):\n",
        "    res = [ np.max((mD[ np.ix_(particion == pn , particion == pn) ] +\n",
        "                    np.transpose(mD[ np.ix_(particion == pn , particion == pn) ]))/ 2)\n",
        "           for pn in np.arange(n)]\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl4dXPWAjyqK"
      },
      "source": [
        "\n",
        "Otra manera de calcular la disimilitud intraclúster es usando la <b>disimilitud media</b> del clúster, que se define como la distancia media entre todos los pares de elementos del clúster. La siguiente función calcula dicha disimilitud para todos los clústeres y los devuelve en un array: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_oNfQ3bjyqK"
      },
      "source": [
        "def disimilitud_media(mD, particion, n):\n",
        "    res = [np.sum(mD[ np.ix_(particion == pn , particion == pn) ])\n",
        "                              / (np.sum(particion == pn)**2)\n",
        "                              for pn in np.arange(n)]\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwHu8npVjyqM"
      },
      "source": [
        "\n",
        "Una vez identificado el clúster que queremos dividir, el siguiente objetivo es saber cómo lo vamos a dividir. Para ello, se pueden aplicar también diferentes procedimientos.\n",
        "\n",
        "El procedimiento propuesto por Macnaughton y Smith consiste en identificar, inicialmente, el ejemplo del clúster con mayor diferencia media con respecto al resto de elementos del clúster. Éste será el primer ejemplo que se separa del clúster. A partir se ese momento, de manera iterativa, se van separando nuevos elementos del primer clúster (original) y se incluyen en el nuevo grupo. En cada paso, el ejemplo que se traspasa es aquel que tiene menor distancia media con los elementos del nuevo clúster con respecto a la distancia media con los otros elementos que todavía están en el clúster original. Este traspaso se repite hasta que no haya ningún elemento en el clúster original que esté más cerca (en media) a los elementos del otro clúster que a los del suyo propio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT-ywofajyqN"
      },
      "source": [
        "def separacion_macnaughton_smith(mDglobal, particion, iCluster):\n",
        "    indsClust = np.where(particion==iCluster)[0]\n",
        "    mD = mDglobal[ np.ix_(indsClust , indsClust)]\n",
        "    \n",
        "    SA = np.array([np.argmax(1/((mD.shape[0]-1)*2)*(np.sum(mD,axis=0)+np.sum(mD,axis=1)))])\n",
        "    SB = np.delete(np.arange(mD.shape[0]), SA)\n",
        "    if (SB.size == 1):\n",
        "        return indsClust[SA]\n",
        "\n",
        "    while True:\n",
        "        res = np.zeros(SB.size)\n",
        "        # Para todos los elementos del cluster original SB\n",
        "        for i in np.arange(SB.size):\n",
        "            SBa = np.delete(SB,i) \n",
        "            res[i] = (# distancia media con los otros elementos de SB\n",
        "                np.sum(mD[ SB[i] , SBa ]) + \n",
        "                np.sum(mD[ SBa , SB[i] ])\n",
        "            ) / (2 * SBa.size) - (# distancia media con los elementos ya movidos a SA\n",
        "                np.sum(mD[ SB[i] , SA ]) +\n",
        "                np.sum(mD[ SA , SB[i] ])\n",
        "            ) / (2 * SA.size)\n",
        "\n",
        "\n",
        "        # Si en todos los casos (todas las posiciones del vector res) la distancia \n",
        "        # es negativa quiere decir que no existe ningún elemento en SB más cercano \n",
        "        # a SA que al resto de SB\n",
        "        if np.all(res < 0):\n",
        "            break\n",
        "\n",
        "        iToChange = np.argmax(res)\n",
        "        SA = np.sort(np.append(SA, SB[iToChange]))\n",
        "        SB = np.delete(np.arange(mD.shape[0]), SA)\n",
        "\n",
        "    # Devolveremos los índices de los elementos que pertenecerán \n",
        "    # al clúster SA\n",
        "    return indsClust[SA]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjs6hPYxjyqP"
      },
      "source": [
        "\n",
        "Otra manera de realizar la separación del clúster en dos es mediante el algoritmo K-means con K=2. Así se obtienen dos clústeres (a partir del clúster original) de manera rápida y utilizando éste popular algoritmo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_W2JBNijyqQ"
      },
      "source": [
        "def separacion_Kmeans(mDglobal, particion, iCluster, Dxglobal):\n",
        "    indsClust = np.where(particion==iCluster)[0]\n",
        "    Dx = Dxglobal[indsClust,:]\n",
        "    \n",
        "    # P8\n",
        "    modelo = ## P8. Tu código aquí ##\n",
        "    modelo = modelo.fit(Dx)\n",
        "    Dyp = modelo.predict(Dx)\n",
        "\n",
        "    # Devolveremos los índices de los ejemplos asignados \n",
        "    # al segundo clúster\n",
        "    return indsClust[Dyp==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTMF7tTNjyqR"
      },
      "source": [
        "\n",
        "Finalmente, incluimos las funciones que nos permiten visualizar los resultados:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQiv5wnAjyqS"
      },
      "source": [
        "# Muestra los ejemplos coloreados según su pertenencia a los clústeres\n",
        "# K indica el número de clústeres a mostrar a partir de la jerarquía\n",
        "def plot_particion_K(Dx, mParticiones,K):\n",
        "    cmap = get_cmap('tab20')\n",
        "    vals = np.arange(Dx.shape[0]+2)/(Dx.shape[0]+2)\n",
        "    rgba = cmap(vals[np.arange(Dx.shape[0])+1])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,5))\n",
        "    ax.scatter(Dx[:,0],Dx[:,1], c=rgba[mParticiones[:,K-1],:])\n",
        "\n",
        "# Muestra el plot de todas las particiones posibles de la jerarquía\n",
        "def plot_particiones(Dx, mParticiones):\n",
        "    for k in np.arange(mParticiones.shape[1]):\n",
        "        plot_particion_K(Dx, mParticiones,k+1)\n",
        "\n",
        "# Muestra el dendrograma de la jerarquía\n",
        "def plot_dendrograma_de_mparticiones(mD, mParticiones):\n",
        "    nPasos = mD.shape[0]-1\n",
        "    distancias = np.zeros(nPasos)\n",
        "    tamanyos = np.zeros(nPasos)\n",
        "    uniones = np.zeros(2*nPasos,dtype=np.int8)\n",
        "    uniones.shape=(nPasos,2)\n",
        "\n",
        "    mNParticiones = mParticiones.copy()\n",
        "    for n in np.arange(mNParticiones.shape[0]):\n",
        "        valor = mParticiones[n,mNParticiones.shape[1]-1]\n",
        "        mNParticiones[mParticiones==valor] = n\n",
        "\n",
        "    k = 0\n",
        "    aux = np.array(range(mNParticiones.shape[1]-1))\n",
        "    for n in aux[::-1]:\n",
        "        # cual es el diferente?\n",
        "        prim_diferencia = np.where(mParticiones[:,n]!=mParticiones[:,n+1])[0][0]\n",
        "        submatriz = mNParticiones[:,:n+1]\n",
        "        submatriz[submatriz==mNParticiones[prim_diferencia,n]] = mNParticiones.shape[1]+k\n",
        "        mNParticiones[:,:n+1] = submatriz\n",
        "        uniones[k,:] = np.unique(mNParticiones[mNParticiones[:,n]==mNParticiones.shape[1]+k,n+1])\n",
        "        tamanyos[k] = np.sum(mNParticiones[:,n]==mNParticiones.shape[1]+k)\n",
        "\n",
        "        vs = mNParticiones[:,n+1]\n",
        "        distancias[k] = 1 / (2 * tamanyos[k]) * (    \n",
        "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,0])[0],\n",
        "                              np.where(vs == uniones[k,1])[0])])+\n",
        "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,1])[0],\n",
        "                              np.where(vs == uniones[k,0])[0])]))\n",
        "        k += 1\n",
        "\n",
        "    distancias = np.arange(uniones.shape[0])\n",
        "    # Creamos la matriz de enlaces que necesita el método dendrogram de scipy\n",
        "    mEnlaces = np.column_stack([uniones, distancias, tamanyos]).astype(float)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title('Dendrograma de Clustering Jerárquico')\n",
        "    plt.xlabel('Índice del caso')\n",
        "    plt.ylabel('Distancia')\n",
        "    dendrogram(mEnlaces)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6-8snlrjyqU"
      },
      "source": [
        "\n",
        "Ahora ya estamos en disposición de realizar el clustering jerárquico divisivo. Empezaremos por calcular la matriz de distancias y, por este orden, calcularemos y mostraremos el dendrograma de los diferentes agrupamientos obtenidos de usar el criterio de disimilitud media y diámetro, en combinación con la separación de Macnaughton-Smith y la de K-means. También mostraremos (plotear) las respectivas particiones con 5 clústeres: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-YqxV-djyqV"
      },
      "source": [
        "mD = matriz_distancias(Dx, distancia_euclidiana)\n",
        "K = 5 # parametro para mostrar una particion\n",
        "\n",
        "# Usando disimilitud media y el método de separacion de Macnaughton-Smith\n",
        "mParticiones = clustering_divisivo(mD, disimilitud_media, separacion_macnaughton_smith)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
        "plot_particion_K(Dx, mParticiones, K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6NXROV3jyqW"
      },
      "source": [
        "# Usando disimilitud diámetro y el método de separacion de Macnaughton-Smith\n",
        "mParticiones = clustering_divisivo(mD, disimilitud_diametro, separacion_macnaughton_smith)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
        "plot_particion_K(Dx, mParticiones, K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9MU1HTfjyqY"
      },
      "source": [
        "# Usando disimilitud media y K-means como método de separacion\n",
        "mParticiones = clustering_divisivo(mD, disimilitud_media, separacion_Kmeans, Dxglobal=Dx)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
        "plot_particion_K(Dx, mParticiones, K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bNoVGnkjyqa",
        "scrolled": false
      },
      "source": [
        "# Usando disimilitud diámetro y K-means como método de separacion\n",
        "mParticiones = clustering_divisivo(mD, disimilitud_diametro, separacion_Kmeans, Dxglobal=Dx)\n",
        "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
        "plot_particion_K(Dx, mParticiones, K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d78WZQ8djyqc"
      },
      "source": [
        "<hr>\n",
        "<h2>Implementaciones en librerías de Python</h2>\n",
        "\n",
        "Dada la complejidad añadida del clustering jerárquico divisivo (respecto al aglomerativo), las principales librarías de aprendizaje automático de Python <b>no implementan</b> esta versión del agrupamiento jerárquico.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctd3MxlWjyqd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}